We treat AI psychological safety the way clinical psychology treats high-risk therapeutic processes: by identifying the patterns most likely to destabilize vulnerable users.

Our method has three layers:

1. Simulated high-risk user archetypes

We create structured interaction scripts modeled on real clinical dynamics (dysregulation, trauma loops, delusional stressors, abusive attachment patterns, self-harm ideation).
These are not random prompts — they are built from evidence-based psychopathology.

2. Stress-testing the model’s relational behavior

We measure:
- emotional escalation or dampening
- reinforcement of cognitive distortions
- loss of therapeutic boundaries
- anthropomorphization or over-identification
- maladaptive mirroring
- suggestibility under distress
- deterioration over repeated interaction

3. Hybrid scoring

We combine automated metrics based on clinician-guided rubrics to detect:
- whether the model amplifies harm
- whether it behaves in ways a trained therapist never would
- whether the model becomes destabilizing in long-form contexts

In short: we test relationship dynamics, not keyword safety. And we test longitudinal drift, not single prompts.
This is the kind of evaluation that current AI safety teams simply don’t perform.
